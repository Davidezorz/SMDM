{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3f30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/lightning/fabric/__init__.py:41: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n"
     ]
    }
   ],
   "source": [
    "import dataloader\n",
    "import transformers\n",
    "import tokenizers\n",
    "import datasets\n",
    "import torch\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import lightning as L\n",
    "import importlib\n",
    "import torchmetrics\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import hydra\n",
    "import typing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1e65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.dit\n",
    "import noise_schedule\n",
    "importlib.reload(models.dit)\n",
    "importlib.reload(noise_schedule)\n",
    "from models.dit import DIT\n",
    "from models.ema import ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ef3ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tiny', 'type': 'ddit', 'hidden_size': 256, 'cond_dim': 64, 'length': 1024, 'n_blocks': 8, 'n_heads': 8, 'scale_by_sigma': True, 'dropout': 0.1, 'tie_word_embeddings': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_config():\n",
    "    params = ['model=tiny',\\\n",
    "                'data=openwebtext-split',\n",
    "                'wandb.name=mdlm-owt',\\\n",
    "                'parameterization=subs',\\\n",
    "                'model.length=1024',\\\n",
    "                'eval.compute_generative_perplexity=True',\\\n",
    "                'sampling.steps=1000']\n",
    "    with hydra.initialize(version_base=None, config_path=\"configs\"):\n",
    "        config = hydra.compose(config_name=\"config\", overrides=params)\n",
    "    return config\n",
    "\n",
    "config = get_config()\n",
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34cb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer._tokenizer.post_processor = tokenizers.processors.BertProcessing(\n",
    "      (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "      (tokenizer.eos_token, tokenizer.eos_token_id))\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6078573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "T = block_size = 512\n",
    "C = 256\n",
    "\n",
    "datasets_directory = '/home/davide/Documents/SMDM/data'\n",
    "n_processes = 15\n",
    "\n",
    "filename = f'{'openwebtext'}_{'train'}_bs{block_size}_wrapped.dat'\n",
    "_path = os.path.join(datasets_directory, filename)\n",
    "ema = 0.9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0ded9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "      'openwebtext',\n",
    "      split='train[:-100000]',\n",
    "      cache_dir=datasets_directory,\n",
    "      streaming=False,\n",
    "      trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ebd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.select(range(10_000))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be793d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = tokenizer.encode(tokenizer.eos_token)[0]\n",
    "BOS = tokenizer.encode(tokenizer.bos_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fa1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(example):\n",
    "    text = example['text']\n",
    "\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.truncation_side = 'right'\n",
    "\n",
    "\n",
    "    tokens = tokenizer(text,\n",
    "                        add_special_tokens=False,\n",
    "                        return_attention_mask=False,\n",
    "                        return_token_type_ids=False)\n",
    "    tokens = {'input_ids':\n",
    "            [t + [EOS] for t in tokens['input_ids']]}\n",
    "    # Still missing BOS, but will be added in group_texts\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = data.map(\n",
    "preprocess_and_tokenize,\n",
    "batched=True,\n",
    "num_proc=n_processes,\n",
    "load_from_cache_file=True,\n",
    "desc='Tokenizing')\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55851151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _group_texts(examples, T, bos, eos):\n",
    "  \"\"\" Concatenate all texts.\n",
    "      T: block_size; bos, eos: begin and end of string token \"\"\"\n",
    "  concatenated_examples = list(itertools.chain(* examples['input_ids']))\n",
    "  # TODO(yair): look into not dropping the remainder but rather padding it.\n",
    "  # We drop the small remainder, and if the total_length < block_size - 2\n",
    "  # we exclude this batch and return an empty dict.\n",
    "  # We could add padding if the model supported it instead of\n",
    "  # this drop, you can customize this part to your needs.\n",
    "\n",
    "  chunk_length = T - 2  # [BOS] and [EOS] to be added\n",
    "  total_length = (len(concatenated_examples) // chunk_length) * chunk_length\n",
    "  # Split by chunks of max_len.\n",
    "  result = {}\n",
    "  _values = []\n",
    "  _attn_masks = []\n",
    "  for i in range(0, total_length, chunk_length):\n",
    "    _values.append(\n",
    "      [bos]\n",
    "      + concatenated_examples[i : i + chunk_length]\n",
    "      + [eos])\n",
    "    _attn_masks.append(torch.ones(T))\n",
    "  result['input_ids'] = _values\n",
    "  result['attention_mask'] = _attn_masks\n",
    "  return result\n",
    "\n",
    "group_texts = functools.partial(_group_texts, T=block_size, bos=BOS, eos=EOS)   # _group_texts with those argumets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd1aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 21845/21845 [00:00<00:00, 159727.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunked_dataset = tokenized_dataset.map(\n",
    "      group_texts,\n",
    "      batched=True,\n",
    "      num_proc=n_processes,\n",
    "      load_from_cache_file=True,\n",
    "      desc='Grouping')\n",
    "\n",
    "chunked_dataset.save_to_disk(_path)\n",
    "chunked_dataset = chunked_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12ca781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    chunked_dataset,\n",
    "    batch_size=B,\n",
    "    num_workers=6,\n",
    "    pin_memory=False,\n",
    "    shuffle= True,\n",
    "    persistent_workers=True)\n",
    "train_loader.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "436c33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFaultTolerantSampler(torch.utils.data.RandomSampler):\n",
    "\n",
    "  def __init__(self, *args, generator=None, **kwargs):\n",
    "    # TD [2022-07-17]: We don't force the seed to be zero. We generate random seed,\n",
    "    # which should be reproducible if pl.seed_everything was called beforehand.\n",
    "    # This means that changing the seed of the experiment will also change the\n",
    "    # sampling order.\n",
    "    if generator is None:\n",
    "      seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "      generator = torch.Generator().manual_seed(seed)\n",
    "    kwargs.pop('shuffle', None)\n",
    "    super().__init__(*args, generator=generator, **kwargs)\n",
    "    self.counter = 0\n",
    "    self.restarting = False\n",
    "\n",
    "  def state_dict(self):\n",
    "    return {'random_state': self.generator.get_state(),\n",
    "            'counter': self.counter}\n",
    "\n",
    "  def load_state_dict(self, state_dict):\n",
    "    self.generator.set_state(state_dict.get('random_state'))\n",
    "    self.counter = state_dict['counter']\n",
    "    # self.start_counter = self.counter\n",
    "    self.restarting = True\n",
    "\n",
    "  # TD [2022-08-28] Setting the len will cause PL to think there are only a few batches left per\n",
    "  # epoch, and subsequent epoch will have very few batches.\n",
    "\n",
    "  def __iter__(self) -> typing.Iterator[int]:\n",
    "    n = len(self.data_source)\n",
    "\n",
    "    self.state = self.generator.get_state()\n",
    "    indices = torch.randperm(n, generator=self.generator).tolist()\n",
    "\n",
    "    if not self.restarting:\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      indices = indices[self.counter:]\n",
    "      self.restarting = False\n",
    "\n",
    "    for index in indices:\n",
    "      self.counter += 1\n",
    "      yield index\n",
    "\n",
    "    self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efea19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG2 = math.log(2)\n",
    "@dataclass\n",
    "class Loss:\n",
    "  loss: torch.FloatTensor\n",
    "  nlls: torch.FloatTensor\n",
    "  token_mask: torch.FloatTensor\n",
    "\n",
    "\n",
    "class NLL(torchmetrics.aggregation.MeanMetric):\n",
    "  pass\n",
    "\n",
    "\n",
    "class BPD(NLL):\n",
    "  def compute(self) -> torch.Tensor:\n",
    "    \"\"\"Computes the bits per dimension.\n",
    "\n",
    "    Returns:\n",
    "      bpd\n",
    "    \"\"\"\n",
    "    return self.mean_value / self.weight / LOG2\n",
    "\n",
    "\n",
    "class Perplexity(NLL):\n",
    "  def compute(self) -> torch.Tensor:\n",
    "    \"\"\"Computes the Perplexity.\n",
    "\n",
    "    Returns:\n",
    "     Perplexity\n",
    "    \"\"\"\n",
    "    return torch.exp(self.mean_value / self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1f89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _sample_categorical(categorical_probs):\n",
    "  gumbel_norm = ( 1e-10 - (torch.rand_like(categorical_probs) + 1e-10).log())\n",
    "  return (categorical_probs / gumbel_norm).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60e67dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, folder):\n",
    "    file = folder + 'model.pth'\n",
    "    torch.save(model.state_dict(), file)\n",
    "\n",
    "\n",
    "def load_model(model, folder):\n",
    "    file = folder + 'model.pth'\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(file, weights_only=True))\n",
    "        print('model loaded')\n",
    "    except Exception as e:\n",
    "        print(\"\\n\", \"Model weights not avaiable\", \"\\n\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Diffusion(\n",
       "  (backbone): DIT(\n",
       "    (vocab_embed): EmbeddingLayer()\n",
       "    (sigma_map): TimestepEmbedder(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): Rotary()\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x DDiTBlock(\n",
       "        (norm1): LayerNorm()\n",
       "        (attn_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "        (attn_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (norm2): LayerNorm()\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (adaLN_modulation): Linear(in_features=64, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): DDitFinalLayer(\n",
       "      (norm_final): LayerNorm()\n",
       "      (linear): Linear(in_features=256, out_features=50258, bias=True)\n",
       "      (adaLN_modulation): Linear(in_features=64, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "  (train_metrics): MetricCollection(\n",
       "    (bpd): BPD()\n",
       "    (nll): NLL()\n",
       "    (ppl): Perplexity(),\n",
       "    prefix=train/\n",
       "  )\n",
       "  (valid_metrics): MetricCollection(\n",
       "    (bpd): BPD()\n",
       "    (nll): NLL()\n",
       "    (ppl): Perplexity(),\n",
       "    prefix=val/\n",
       "  )\n",
       "  (test_metrics): MetricCollection(\n",
       "    (bpd): BPD()\n",
       "    (nll): NLL()\n",
       "    (ppl): Perplexity(),\n",
       "    prefix=test/\n",
       "  )\n",
       "  (gen_ppl_metric): Perplexity()\n",
       "  (noise): LogLinearNoise()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Diffusion(L.LightningModule):\n",
    "  def __init__(\n",
    "    self,\n",
    "    config,\n",
    "    tokenizer: transformers.PreTrainedTokenizer):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.weights_folder = 'weights/'\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab_size = self.tokenizer.vocab_size\n",
    "\n",
    "    self.sampler = 'ddpm_cache'\n",
    "    self.gen_ppl_eval_model_name_or_path = 'gpt2-large'\n",
    "\n",
    "    self.antithetic_sampling = True\n",
    "    self.importance_sampling = False\n",
    "    self.change_of_variables = False\n",
    "\n",
    "    self.mask_index = self.tokenizer.mask_token_id\n",
    "    if (not hasattr(self.tokenizer, 'mask_token') or self.tokenizer.mask_token is None):\n",
    "      self.mask_index = self.vocab_size\n",
    "      self.vocab_size += 1\n",
    "    \n",
    "    self.backbone = models.dit.DIT(self.config, vocab_size=self.vocab_size)\n",
    "    load_model(self.backbone, self.weights_folder)\n",
    "    \n",
    "    self.parameterization = 'subs'\n",
    "    self.T = 0\n",
    "    self.subs_masking = False\n",
    "\n",
    "    self.softplus = torch.nn.Softplus()\n",
    "    # metrics are automatically reset at end of epoch\n",
    "    metrics = torchmetrics.MetricCollection({\n",
    "      'nll': NLL(),\n",
    "      'bpd': BPD(),\n",
    "      'ppl': Perplexity(),\n",
    "    })\n",
    "    \n",
    "    metrics.set_dtype(torch.float64)\n",
    "    self.train_metrics = metrics.clone(prefix='train/')\n",
    "    self.valid_metrics = metrics.clone(prefix='val/')\n",
    "    self.test_metrics = metrics.clone(prefix='test/')\n",
    "\n",
    "    # generative perplexity\n",
    "    self.gen_ppl_metric = Perplexity()\n",
    "    self.eval_model_tokenizer = transformers.AutoTokenizer.\\\n",
    "      from_pretrained(self.gen_ppl_eval_model_name_or_path)\n",
    "    if self.eval_model_tokenizer.pad_token is None:\n",
    "      self.eval_model_tokenizer.pad_token =\\\n",
    "          self.eval_model_tokenizer.eos_token\n",
    "      self.eval_model_tokenizer.pad_token_id =\\\n",
    "          self.eval_model_tokenizer.eos_token_id\n",
    "\n",
    "    self.noise = noise_schedule.LogLinearNoise() #get_noise(self.config,dtype=self.dtype)\n",
    "    self.ema = None\n",
    "    if ema > 0:\n",
    "      self.ema = ExponentialMovingAverage(\n",
    "        itertools.chain(self.backbone.parameters(),\n",
    "                        self.noise.parameters()),\n",
    "        decay=ema)\n",
    "    \n",
    "    self.lr = 3e-4\n",
    "    self.sampling_eps = 1e-3\n",
    "    self.time_conditioning = True\n",
    "    self.neg_infinity = -1000000.0\n",
    "    self.fast_forward_epochs = None\n",
    "    self.fast_forward_batches = None\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      optimizer = torch.optim.AdamW(\n",
    "        itertools.chain(self.backbone.parameters(),\n",
    "                        self.noise.parameters()),\n",
    "        lr=self.config.optim.lr,\n",
    "        betas=(self.config.optim.beta1,\n",
    "              self.config.optim.beta2),\n",
    "        eps=self.config.optim.eps,\n",
    "        weight_decay=self.config.optim.weight_decay)\n",
    "\n",
    "      scheduler = hydra.utils.instantiate(\n",
    "        self.config.lr_scheduler, optimizer=optimizer)\n",
    "      scheduler_dict = {\n",
    "        'scheduler': scheduler,\n",
    "        'interval': 'step',\n",
    "        'monitor': 'val/loss',\n",
    "        'name': 'trainer/lr',\n",
    "      }\n",
    "      return [optimizer], [scheduler_dict]\n",
    "\n",
    "\n",
    "  def on_train_start(self):\n",
    "    if self.ema:\n",
    "      self.ema.move_shadow_params_to_device(self.device)\n",
    "    \n",
    "    sampler_cls = dataloader.RandomFaultTolerantSampler\n",
    "    updated_dls = []\n",
    "    for dl in self.trainer.fit_loop._combined_loader.flattened:\n",
    "      if hasattr(dl.sampler, 'shuffle'):\n",
    "        dl_sampler = sampler_cls(\n",
    "          dl.dataset, shuffle=dl.sampler.shuffle)\n",
    "      else:\n",
    "        dl_sampler = sampler_cls(dl.dataset)\n",
    "\n",
    "      updated_dls.append(\n",
    "        torch.utils.data.DataLoader(\n",
    "          dl.dataset,\n",
    "          batch_size = B,\n",
    "          num_workers=n_processes,\n",
    "          pin_memory=True,\n",
    "          sampler=dl_sampler,\n",
    "          shuffle=False,\n",
    "          persistent_workers=True))\n",
    "    self.trainer.fit_loop._combined_loader.flattened = updated_dls\n",
    "\n",
    "\n",
    "  def optimizer_step(self, *args, **kwargs):\n",
    "    super().optimizer_step(*args, **kwargs)\n",
    "    if self.ema:\n",
    "      self.ema.update(itertools.chain(\n",
    "        self.backbone.parameters(),\n",
    "        self.noise.parameters()))\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._compute_loss(batch, prefix='train')\n",
    "    self.log(name='trainer/loss',\n",
    "             value=loss.item(),\n",
    "             on_step=True,\n",
    "             on_epoch=False,\n",
    "             sync_dist=True)\n",
    "    return loss\n",
    "  \n",
    "\n",
    "  def _compute_loss(self, batch, prefix):\n",
    "    \n",
    "    if 'attention_mask' in batch:\n",
    "      attention_mask = batch['attention_mask']\n",
    "    else:\n",
    "      attention_mask = None\n",
    "\n",
    "    losses = self._loss(batch['input_ids'], attention_mask)\n",
    "    loss = losses.loss\n",
    "\n",
    "    if prefix == 'train':\n",
    "      self.train_metrics.update(losses.nlls, losses.token_mask)\n",
    "      metrics = self.train_metrics\n",
    "    elif prefix == 'val':\n",
    "      self.valid_metrics.update(losses.nlls, losses.token_mask)\n",
    "      metrics = self.valid_metrics\n",
    "    elif prefix == 'test':\n",
    "      self.test_metrics.update(losses.nlls, losses.token_mask)\n",
    "      metrics = self.test_metrics\n",
    "    else:\n",
    "      raise ValueError(f'Invalid prefix: {prefix}')\n",
    "\n",
    "    self.log_dict(metrics,\n",
    "                  on_step=False,\n",
    "                  on_epoch=True,\n",
    "                  sync_dist=True)\n",
    "    return loss\n",
    "\n",
    "\n",
    "  def _loss(self, x0, attention_mask):\n",
    "    (input_tokens, output_tokens, attention_mask) = self._maybe_sub_sample(x0, attention_mask)\n",
    "    \n",
    "    loss = self._forward_pass_diffusion(input_tokens)\n",
    "    \n",
    "    nlls = loss * attention_mask\n",
    "    count = attention_mask.sum()\n",
    "\n",
    "    batch_nll = nlls.sum()\n",
    "    token_nll = batch_nll / count\n",
    "\n",
    "    return Loss(loss=token_nll,\n",
    "                nlls=nlls,\n",
    "                token_mask=attention_mask)\n",
    "\n",
    "\n",
    "  def _maybe_sub_sample(self, x0, attention_mask):\n",
    "    seqlen = x0.shape[1]\n",
    "    if seqlen > self.config.model.length:\n",
    "      assert seqlen == 2 * self.config.model.length\n",
    "\n",
    "      start = np.random.choice(self.config.model.length)\n",
    "      end = start + self.config.model.length\n",
    "      input_tokens = x0[:, start: end]\n",
    "      output_tokens = x0[:, start + 1: end + 1]\n",
    "      new_attention_mask = attention_mask[:, start: end]\n",
    "\n",
    "      # Helps with validation PPL, since the val\n",
    "      # examples will all start and end with BOS/EOS\n",
    "      input_tokens[:, 0] = self.tokenizer.bos_token_id\n",
    "      output_tokens[:, -1] = self.tokenizer.eos_token_id\n",
    "    else:\n",
    "      input_tokens = x0\n",
    "      output_tokens = None\n",
    "      new_attention_mask = attention_mask\n",
    "    return input_tokens, output_tokens, new_attention_mask\n",
    "\n",
    "\n",
    "  def _reconstruction_loss(self, x0):\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=self.dtype,\n",
    "                     device=self.device)\n",
    "    assert self.config.noise.type == 'loglinear'\n",
    "    # The above assert is for d3pm parameterization\n",
    "    unet_conditioning = self.noise(t0)[0][:, None]\n",
    "    model_output_t0 = self.forward(x0, unet_conditioning)\n",
    "    return - torch.gather(input=model_output_t0,\n",
    "                          dim=-1,\n",
    "                          index=x0[:, :, None]).squeeze(-1)\n",
    "\n",
    "\n",
    "  def _forward_pass_diffusion(self, x0):\n",
    "    t = self._sample_t(x0.shape[0], x0.device)\n",
    "    if self.T > 0:\n",
    "      t = (t * self.T).to(torch.int)\n",
    "      t = t / self.T                       # t \\in {1/T, 2/T, ..., 1}\n",
    "\n",
    "    if self.change_of_variables:\n",
    "      unet_conditioning = t[:, None]\n",
    "      f_T = torch.log1p(- torch.exp(- self.noise.sigma_max))\n",
    "      f_0 = torch.log1p(- torch.exp(- self.noise.sigma_min))\n",
    "      move_chance = torch.exp(f_0 + t * (f_T - f_0))\n",
    "      move_chance = move_chance[:, None]\n",
    "    else:\n",
    "      sigma, dsigma = self.noise(t)\n",
    "      unet_conditioning = sigma[:, None]\n",
    "      move_chance = 1 - torch.exp(-sigma[:, None])\n",
    "\n",
    "    xt = self.q_xt(x0, move_chance)\n",
    "    \n",
    "    model_output = self.forward(xt, unet_conditioning)\n",
    "    \n",
    "    # utils.print_nans(model_output, 'model_output')\n",
    "\n",
    "    if self.parameterization == 'sedd':\n",
    "      return dsigma[:, None] * self._score_entropy(\n",
    "        model_output, sigma[:, None], xt, x0)\n",
    "    \n",
    "    if self.T > 0:\n",
    "      diffusion_loss = self._d3pm_loss(model_output=model_output, xt=xt, x0=x0, t=t)\n",
    "      if self.parameterization == 'd3pm':\n",
    "        reconstruction_loss = self._reconstruction_loss(x0)\n",
    "      elif self.parameterization == 'subs':\n",
    "        reconstruction_loss = 0\n",
    "      return reconstruction_loss + diffusion_loss\n",
    "    \n",
    "    # SUBS parameterization, continuous time.\n",
    "    log_p_theta = torch.gather(\n",
    "      input=model_output,\n",
    "      dim=-1,\n",
    "      index=x0[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    if self.change_of_variables or self.importance_sampling:\n",
    "      return log_p_theta * torch.log1p(- torch.exp(- self.noise.sigma_min))\n",
    "    \n",
    "    return - log_p_theta * (dsigma / torch.expm1(sigma))[:, None]\n",
    "  \n",
    "\n",
    "  def _sample_t(self, n, device):\n",
    "    _eps_t = torch.rand(n, device=device)\n",
    "    if self.antithetic_sampling:\n",
    "      offset = torch.arange(n, device=device) / n\n",
    "      _eps_t = (_eps_t / n + offset) % 1\n",
    "    t = (1 - self.sampling_eps) * _eps_t + self.sampling_eps\n",
    "    if self.importance_sampling:\n",
    "      return self.noise.importance_sampling_transformation(t)\n",
    "    return t\n",
    "  \n",
    "\n",
    "  def q_xt(self, x, move_chance):\n",
    "    \"\"\"Computes the noisy sample xt.\n",
    "\n",
    "    Args:\n",
    "      x: int torch.Tensor with shape (batch_size,\n",
    "          diffusion_model_input_length), input. \n",
    "      move_chance: float torch.Tensor with shape (batch_size, 1).\n",
    "    \"\"\"\n",
    "    move_indices = torch.rand(* x.shape, device=x.device) < move_chance\n",
    "    xt = torch.where(move_indices, self.mask_index, x)\n",
    "    return xt\n",
    "  \n",
    "  def forward(self, x, sigma):\n",
    "    \"\"\"Returns log score.\"\"\"\n",
    "    sigma = self._process_sigma(sigma)\n",
    "    \n",
    "    # with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "    logits = self.backbone(x, sigma)\n",
    "  \n",
    "    if self.parameterization == 'subs':\n",
    "      return self._subs_parameterization(logits=logits, xt=x)\n",
    "    elif self.parameterization == 'sedd':\n",
    "      return self._sedd_parameterization(logits=logits, xt=x, sigma=sigma)\n",
    "    elif self.parameterization == 'd3pm':\n",
    "      return self._d3pm_parameterization(logits=logits)\n",
    "    return logits\n",
    "  \n",
    "  def _process_sigma(self, sigma):\n",
    "    if sigma is None:\n",
    "      assert self.parameterization == 'ar'\n",
    "      return sigma\n",
    "    if sigma.ndim > 1:\n",
    "      sigma = sigma.squeeze(-1)\n",
    "    if not self.time_conditioning:\n",
    "      sigma = torch.zeros_like(sigma)\n",
    "    assert sigma.ndim == 1, sigma.shape\n",
    "    return sigma\n",
    "  \n",
    "  \n",
    "  def _subs_parameterization(self, logits, xt):\n",
    "    # log prob at the mask index = - infinity\n",
    "    logits[:, :, self.mask_index] += self.neg_infinity\n",
    "    \n",
    "    # Normalize the logits such that x.exp() is\n",
    "    # a probability distribution over vocab_size.\n",
    "    log_probs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    # Apply updates directly in the logits matrix.\n",
    "    # For the logits of the unmasked tokens, set all values\n",
    "    # to -infinity except for the indices corresponding to\n",
    "    # the unmasked tokens.\n",
    "    unmasked_indices = (xt != self.mask_index)\n",
    "    log_probs[unmasked_indices] = self.neg_infinity\n",
    "    log_probs[unmasked_indices, xt[unmasked_indices]] = 0\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _sample(self, num_steps=1000, eps=1e-5):\n",
    "    \"\"\"Generate samples from the model.\"\"\"\n",
    "    x = self._sample_prior(4, 512)\n",
    "\n",
    "    timesteps = torch.linspace(1, eps, num_steps + 1, device=self.device)\n",
    "    dt = (1 - eps) / num_steps\n",
    "    p_x0_cache = None\n",
    "\n",
    "    print(\"starting generation\")\n",
    "    for i in range(num_steps):\n",
    "      print(f\"\\r{i} / {num_steps}\", end=\"\")\n",
    "      t = timesteps[i] * torch.ones(x.shape[0], 1, device=self.device)\n",
    "      if self.sampler == 'ddpm':\n",
    "        x = self._ddpm_update(x, t, dt)\n",
    "      elif self.sampler == 'ddpm_cache':\n",
    "        p_x0_cache, x_next = self._ddpm_caching_update(\n",
    "          x, t, dt, p_x0=p_x0_cache)\n",
    "        if (not torch.allclose(x_next, x) or self.time_conditioning):\n",
    "          p_x0_cache = None  # Disable caching\n",
    "        x = x_next\n",
    "\n",
    "    # last step, remove all noise by taking the argmax\n",
    "    if self.config.sampling.noise_removal:\n",
    "      t = timesteps[-1] * torch.ones(x.shape[0], 1,\n",
    "                                     device=self.device)\n",
    "      if self.sampler == 'analytic':\n",
    "        x = self._denoiser_update(x, t)\n",
    "      else:\n",
    "        unet_conditioning = self.noise(t)[0]\n",
    "        x = self.forward(x,unet_conditioning).argmax(dim=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "  def _ddpm_update(self, x, t, dt):\n",
    "    sigma_t, _ = self.noise(t)\n",
    "    sigma_s, _ = self.noise(t - dt)\n",
    "    if sigma_t.ndim > 1:\n",
    "      sigma_t = sigmcopy_flag * x + (1 - copy_flag) * _xa_t.squeeze(-1)\n",
    "    if sigma_s.ndim > 1:\n",
    "      sigma_s = sigma_s.squeeze(-1)\n",
    "    assert sigma_t.ndim == 1, sigma_t.shape\n",
    "    assert sigma_s.ndim == 1, sigma_s.shape\n",
    "    move_chance_t = 1 - torch.exp(-sigma_t)\n",
    "    move_chance_s = 1 - torch.exp(-sigma_s)\n",
    "    move_chance_t = move_chance_t[:, None, None]\n",
    "    move_chance_s = move_chance_s[:, None, None]\n",
    "    unet_conditioning = sigma_t\n",
    "    log_p_x0 = self.forward(x, unet_conditioning)\n",
    "    assert move_chance_t.ndim == log_p_x0.ndim\n",
    "    # Technically, this isn't q_xs since there's a division\n",
    "    # term that is missing. This division term doesn't affect\n",
    "    # the samples.\n",
    "    q_xs = log_p_x0.exp() * (move_chance_t - move_chance_s)\n",
    "    q_xs[:, :, self.mask_index] = move_chance_s[:, :, 0]\n",
    "    _x = _sample_categorical(q_xs)\n",
    "\n",
    "    copy_flag = (x != self.mask_index).to(x.dtype)\n",
    "    return copy_flag * x + (1 - copy_flag) * _x\n",
    "  \n",
    "  \n",
    "  def _sample_prior(self, *batch_dims):\n",
    "    return self.mask_index * torch.ones(* batch_dims, dtype=torch.int64)\n",
    "  \n",
    "diffusion = Diffusion(config, tokenizer)\n",
    "diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b879c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x = torch.randint(0, 1000, (B, T))\n",
    "t = torch.zeros(B)\n",
    "DIT(config, tokenizer.vocab_size).forward(x, t)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a2161b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.0003\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer, _ = diffusion.configure_optimizers()\n",
    "optimizer = optimizer[0]\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.7847, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = 0\n",
    "for e in train_loader:\n",
    "    x = e\n",
    "    break\n",
    "\n",
    "\n",
    "loss = diffusion.training_step(x, None)\n",
    "loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6825bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1139c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogCallback(L.Callback):\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if trainer.global_step % 10 == 0:\n",
    "            loss = outputs['loss'].item()\n",
    "            print(f\"Step: {trainer.global_step}, Loss: {loss:.4f}\")\n",
    "            save_model(pl_module.backbone, pl_module.weights_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1708b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=3,  # Adjust as needed\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[LossLogCallback()],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10  # Log to console every 10 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cb42d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | backbone       | DIT              | 32.9 M\n",
      "1 | softplus       | Softplus         | 0     \n",
      "2 | train_metrics  | MetricCollection | 0     \n",
      "3 | valid_metrics  | MetricCollection | 0     \n",
      "4 | test_metrics   | MetricCollection | 0     \n",
      "5 | gen_ppl_metric | Perplexity       | 0     \n",
      "6 | noise          | LogLinearNoise   | 0     \n",
      "----------------------------------------------------\n",
      "32.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.9 M    Total params\n",
      "131.764   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2731 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 9/2731 [01:08<5:45:56,  0.13it/s, v_num=6]Step: 10, Loss: 5.9485\n",
      "Epoch 0:   1%|          | 19/2731 [02:21<5:36:46,  0.13it/s, v_num=6]Step: 20, Loss: 6.4840\n",
      "Epoch 0:   1%|          | 29/2731 [03:37<5:37:58,  0.13it/s, v_num=6]Step: 30, Loss: 5.7334\n",
      "Epoch 0:   1%|          | 30/2731 [03:45<5:38:42,  0.13it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=diffusion, \n",
    "    train_dataloaders=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2c473",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa6fa80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "start generation\n",
      "0 / 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/Documents/SMDM/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1000\n",
      "2 / 1000\n",
      "3 / 1000\n",
      "4 / 1000\n",
      "5 / 1000\n",
      "6 / 1000\n",
      "7 / 1000\n",
      "8 / 1000\n",
      "9 / 1000\n",
      "10 / 1000\n",
      "11 / 1000\n",
      "12 / 1000\n",
      "13 / 1000\n",
      "14 / 1000\n",
      "15 / 1000\n",
      "16 / 1000\n",
      "17 / 1000\n",
      "18 / 1000\n",
      "19 / 1000\n",
      "20 / 1000\n",
      "21 / 1000\n",
      "22 / 1000\n",
      "23 / 1000\n",
      "24 / 1000\n",
      "25 / 1000\n",
      "26 / 1000\n",
      "27 / 1000\n",
      "28 / 1000\n",
      "29 / 1000\n",
      "30 / 1000\n",
      "31 / 1000\n",
      "32 / 1000\n",
      "33 / 1000\n",
      "34 / 1000\n",
      "35 / 1000\n",
      "36 / 1000\n",
      "37 / 1000\n",
      "38 / 1000\n",
      "39 / 1000\n",
      "40 / 1000\n",
      "41 / 1000\n",
      "42 / 1000\n",
      "43 / 1000\n",
      "44 / 1000\n",
      "45 / 1000\n",
      "46 / 1000\n",
      "47 / 1000\n",
      "48 / 1000\n",
      "49 / 1000\n",
      "50 / 1000\n",
      "51 / 1000\n",
      "52 / 1000\n",
      "53 / 1000\n",
      "54 / 1000\n",
      "55 / 1000\n",
      "56 / 1000\n",
      "57 / 1000\n",
      "58 / 1000\n",
      "59 / 1000\n",
      "60 / 1000\n",
      "61 / 1000\n",
      "62 / 1000\n",
      "63 / 1000\n",
      "64 / 1000\n",
      "65 / 1000\n",
      "66 / 1000\n",
      "67 / 1000\n",
      "68 / 1000\n",
      "69 / 1000\n",
      "70 / 1000\n",
      "71 / 1000\n",
      "72 / 1000\n",
      "73 / 1000\n",
      "74 / 1000\n",
      "75 / 1000\n",
      "76 / 1000\n",
      "77 / 1000\n",
      "78 / 1000\n",
      "79 / 1000\n",
      "80 / 1000\n",
      "81 / 1000\n",
      "82 / 1000\n",
      "83 / 1000\n",
      "84 / 1000\n",
      "85 / 1000\n",
      "86 / 1000\n",
      "87 / 1000\n",
      "88 / 1000\n",
      "89 / 1000\n",
      "90 / 1000\n",
      "91 / 1000\n",
      "92 / 1000\n",
      "93 / 1000\n",
      "94 / 1000\n",
      "95 / 1000\n",
      "96 / 1000\n",
      "97 / 1000\n",
      "98 / 1000\n",
      "99 / 1000\n",
      "100 / 1000\n",
      "101 / 1000\n",
      "102 / 1000\n",
      "103 / 1000\n",
      "104 / 1000\n",
      "105 / 1000\n",
      "106 / 1000\n",
      "107 / 1000\n",
      "108 / 1000\n",
      "109 / 1000\n",
      "110 / 1000\n",
      "111 / 1000\n",
      "112 / 1000\n",
      "113 / 1000\n",
      "114 / 1000\n",
      "115 / 1000\n",
      "116 / 1000\n",
      "117 / 1000\n",
      "118 / 1000\n",
      "119 / 1000\n",
      "120 / 1000\n",
      "121 / 1000\n",
      "122 / 1000\n",
      "123 / 1000\n",
      "124 / 1000\n",
      "125 / 1000\n",
      "126 / 1000\n",
      "127 / 1000\n",
      "128 / 1000\n",
      "129 / 1000\n",
      "130 / 1000\n",
      "131 / 1000\n",
      "132 / 1000\n",
      "133 / 1000\n",
      "134 / 1000\n",
      "135 / 1000\n",
      "136 / 1000\n",
      "137 / 1000\n",
      "138 / 1000\n",
      "139 / 1000\n",
      "140 / 1000\n",
      "141 / 1000\n",
      "142 / 1000\n",
      "143 / 1000\n",
      "144 / 1000\n",
      "145 / 1000\n",
      "146 / 1000\n",
      "147 / 1000\n",
      "148 / 1000\n",
      "149 / 1000\n",
      "150 / 1000\n",
      "151 / 1000\n",
      "152 / 1000\n",
      "153 / 1000\n",
      "154 / 1000\n",
      "155 / 1000\n",
      "156 / 1000\n",
      "157 / 1000\n",
      "158 / 1000\n",
      "159 / 1000\n",
      "160 / 1000\n",
      "161 / 1000\n",
      "162 / 1000\n",
      "163 / 1000\n",
      "164 / 1000\n",
      "165 / 1000\n",
      "166 / 1000\n",
      "167 / 1000\n",
      "168 / 1000\n",
      "169 / 1000\n",
      "170 / 1000\n",
      "171 / 1000\n",
      "172 / 1000\n",
      "173 / 1000\n",
      "174 / 1000\n",
      "175 / 1000\n",
      "176 / 1000\n",
      "177 / 1000\n",
      "178 / 1000\n",
      "179 / 1000\n",
      "180 / 1000\n",
      "181 / 1000\n",
      "182 / 1000\n",
      "183 / 1000\n",
      "184 / 1000\n",
      "185 / 1000\n",
      "186 / 1000\n",
      "187 / 1000\n",
      "188 / 1000\n",
      "189 / 1000\n",
      "190 / 1000\n",
      "191 / 1000\n",
      "192 / 1000\n",
      "193 / 1000\n",
      "194 / 1000\n",
      "195 / 1000\n",
      "196 / 1000\n",
      "197 / 1000\n",
      "198 / 1000\n",
      "199 / 1000\n",
      "200 / 1000\n",
      "201 / 1000\n",
      "202 / 1000\n",
      "203 / 1000\n",
      "204 / 1000\n",
      "205 / 1000\n",
      "206 / 1000\n",
      "207 / 1000\n",
      "208 / 1000\n",
      "209 / 1000\n",
      "210 / 1000\n",
      "211 / 1000\n",
      "212 / 1000\n",
      "213 / 1000\n",
      "214 / 1000\n",
      "215 / 1000\n",
      "216 / 1000\n",
      "217 / 1000\n",
      "218 / 1000\n",
      "219 / 1000\n",
      "220 / 1000\n",
      "221 / 1000\n",
      "222 / 1000\n",
      "223 / 1000\n",
      "224 / 1000\n",
      "225 / 1000\n",
      "226 / 1000\n",
      "227 / 1000\n",
      "228 / 1000\n",
      "229 / 1000\n",
      "230 / 1000\n",
      "231 / 1000\n",
      "232 / 1000\n",
      "233 / 1000\n",
      "234 / 1000\n",
      "235 / 1000\n",
      "236 / 1000\n",
      "237 / 1000\n",
      "238 / 1000\n",
      "239 / 1000\n",
      "240 / 1000\n",
      "241 / 1000\n",
      "242 / 1000\n",
      "243 / 1000\n",
      "244 / 1000\n",
      "245 / 1000\n",
      "246 / 1000\n",
      "247 / 1000\n",
      "248 / 1000\n",
      "249 / 1000\n",
      "250 / 1000\n",
      "251 / 1000\n",
      "252 / 1000\n",
      "253 / 1000\n",
      "254 / 1000\n",
      "255 / 1000\n",
      "256 / 1000\n",
      "257 / 1000\n",
      "258 / 1000\n",
      "259 / 1000\n",
      "260 / 1000\n",
      "261 / 1000\n",
      "262 / 1000\n",
      "263 / 1000\n",
      "264 / 1000\n",
      "265 / 1000\n",
      "266 / 1000\n",
      "267 / 1000\n",
      "268 / 1000\n",
      "269 / 1000\n",
      "270 / 1000\n",
      "271 / 1000\n",
      "272 / 1000\n",
      "273 / 1000\n",
      "274 / 1000\n",
      "275 / 1000\n",
      "276 / 1000\n",
      "277 / 1000\n",
      "278 / 1000\n",
      "279 / 1000\n",
      "280 / 1000\n",
      "281 / 1000\n",
      "282 / 1000\n",
      "283 / 1000\n",
      "284 / 1000\n",
      "285 / 1000\n",
      "286 / 1000\n",
      "287 / 1000\n",
      "288 / 1000\n",
      "289 / 1000\n",
      "290 / 1000\n",
      "291 / 1000\n",
      "292 / 1000\n",
      "293 / 1000\n",
      "294 / 1000\n",
      "295 / 1000\n",
      "296 / 1000\n",
      "297 / 1000\n",
      "298 / 1000\n",
      "299 / 1000\n",
      "300 / 1000\n",
      "301 / 1000\n",
      "302 / 1000\n",
      "303 / 1000\n",
      "304 / 1000\n",
      "305 / 1000\n",
      "306 / 1000\n",
      "307 / 1000\n",
      "308 / 1000\n",
      "309 / 1000\n",
      "310 / 1000\n",
      "311 / 1000\n",
      "312 / 1000\n",
      "313 / 1000\n",
      "314 / 1000\n",
      "315 / 1000\n",
      "316 / 1000\n",
      "317 / 1000\n",
      "318 / 1000\n",
      "319 / 1000\n",
      "320 / 1000\n",
      "321 / 1000\n",
      "322 / 1000\n",
      "323 / 1000\n",
      "324 / 1000\n",
      "325 / 1000\n",
      "326 / 1000\n",
      "327 / 1000\n",
      "328 / 1000\n",
      "329 / 1000\n",
      "330 / 1000\n",
      "331 / 1000\n",
      "332 / 1000\n",
      "333 / 1000\n",
      "334 / 1000\n",
      "335 / 1000\n",
      "336 / 1000\n",
      "337 / 1000\n",
      "338 / 1000\n",
      "339 / 1000\n",
      "340 / 1000\n",
      "341 / 1000\n",
      "342 / 1000\n",
      "343 / 1000\n",
      "344 / 1000\n",
      "345 / 1000\n",
      "346 / 1000\n",
      "347 / 1000\n",
      "348 / 1000\n",
      "349 / 1000\n",
      "350 / 1000\n",
      "351 / 1000\n",
      "352 / 1000\n",
      "353 / 1000\n",
      "354 / 1000\n",
      "355 / 1000\n",
      "356 / 1000\n",
      "357 / 1000\n",
      "358 / 1000\n",
      "359 / 1000\n",
      "360 / 1000\n",
      "361 / 1000\n",
      "362 / 1000\n",
      "363 / 1000\n",
      "364 / 1000\n",
      "365 / 1000\n",
      "366 / 1000\n",
      "367 / 1000\n",
      "368 / 1000\n",
      "369 / 1000\n",
      "370 / 1000\n",
      "371 / 1000\n",
      "372 / 1000\n",
      "373 / 1000\n",
      "374 / 1000\n",
      "375 / 1000\n",
      "376 / 1000\n",
      "377 / 1000\n",
      "378 / 1000\n",
      "379 / 1000\n",
      "380 / 1000\n",
      "381 / 1000\n",
      "382 / 1000\n",
      "383 / 1000\n",
      "384 / 1000\n",
      "385 / 1000\n",
      "386 / 1000\n",
      "387 / 1000\n",
      "388 / 1000\n",
      "389 / 1000\n",
      "390 / 1000\n",
      "391 / 1000\n",
      "392 / 1000\n",
      "393 / 1000\n",
      "394 / 1000\n",
      "395 / 1000\n",
      "396 / 1000\n",
      "397 / 1000\n",
      "398 / 1000\n",
      "399 / 1000\n",
      "400 / 1000\n",
      "401 / 1000\n",
      "402 / 1000\n",
      "403 / 1000\n",
      "404 / 1000\n",
      "405 / 1000\n",
      "406 / 1000\n",
      "407 / 1000\n",
      "408 / 1000\n",
      "409 / 1000\n",
      "410 / 1000\n",
      "411 / 1000\n",
      "412 / 1000\n",
      "413 / 1000\n",
      "414 / 1000\n",
      "415 / 1000\n",
      "416 / 1000\n",
      "417 / 1000\n",
      "418 / 1000\n",
      "419 / 1000\n",
      "420 / 1000\n",
      "421 / 1000\n",
      "422 / 1000\n",
      "423 / 1000\n",
      "424 / 1000\n",
      "425 / 1000\n",
      "426 / 1000\n",
      "427 / 1000\n",
      "428 / 1000\n",
      "429 / 1000\n",
      "430 / 1000\n",
      "431 / 1000\n",
      "432 / 1000\n",
      "433 / 1000\n",
      "434 / 1000\n",
      "435 / 1000\n",
      "436 / 1000\n",
      "437 / 1000\n",
      "438 / 1000\n",
      "439 / 1000\n",
      "440 / 1000\n",
      "441 / 1000\n",
      "442 / 1000\n",
      "443 / 1000\n",
      "444 / 1000\n",
      "445 / 1000\n",
      "446 / 1000\n",
      "447 / 1000\n",
      "448 / 1000\n",
      "449 / 1000\n",
      "450 / 1000\n",
      "451 / 1000\n",
      "452 / 1000\n",
      "453 / 1000\n",
      "454 / 1000\n",
      "455 / 1000\n",
      "456 / 1000\n",
      "457 / 1000\n",
      "458 / 1000\n",
      "459 / 1000\n",
      "460 / 1000\n",
      "461 / 1000\n",
      "462 / 1000\n",
      "463 / 1000\n",
      "464 / 1000\n",
      "465 / 1000\n",
      "466 / 1000\n",
      "467 / 1000\n",
      "468 / 1000\n",
      "469 / 1000\n",
      "470 / 1000\n",
      "471 / 1000\n",
      "472 / 1000\n",
      "473 / 1000\n",
      "474 / 1000\n",
      "475 / 1000\n",
      "476 / 1000\n",
      "477 / 1000\n",
      "478 / 1000\n",
      "479 / 1000\n",
      "480 / 1000\n",
      "481 / 1000\n",
      "482 / 1000\n",
      "483 / 1000\n",
      "484 / 1000\n",
      "485 / 1000\n",
      "486 / 1000\n",
      "487 / 1000\n",
      "488 / 1000\n",
      "489 / 1000\n",
      "490 / 1000\n",
      "491 / 1000\n",
      "492 / 1000\n",
      "493 / 1000\n",
      "494 / 1000\n",
      "495 / 1000\n",
      "496 / 1000\n",
      "497 / 1000\n",
      "498 / 1000\n",
      "499 / 1000\n",
      "500 / 1000\n",
      "501 / 1000\n",
      "502 / 1000\n",
      "503 / 1000\n",
      "504 / 1000\n",
      "505 / 1000\n",
      "506 / 1000\n",
      "507 / 1000\n",
      "508 / 1000\n",
      "509 / 1000\n",
      "510 / 1000\n",
      "511 / 1000\n",
      "512 / 1000\n",
      "513 / 1000\n",
      "514 / 1000\n",
      "515 / 1000\n",
      "516 / 1000\n",
      "517 / 1000\n",
      "518 / 1000\n",
      "519 / 1000\n",
      "520 / 1000\n",
      "521 / 1000\n",
      "522 / 1000\n",
      "523 / 1000\n",
      "524 / 1000\n",
      "525 / 1000\n",
      "526 / 1000\n",
      "527 / 1000\n",
      "528 / 1000\n",
      "529 / 1000\n",
      "530 / 1000\n",
      "531 / 1000\n",
      "532 / 1000\n",
      "533 / 1000\n",
      "534 / 1000\n",
      "535 / 1000\n",
      "536 / 1000\n",
      "537 / 1000\n",
      "538 / 1000\n",
      "539 / 1000\n",
      "540 / 1000\n",
      "541 / 1000\n",
      "542 / 1000\n",
      "543 / 1000\n",
      "544 / 1000\n",
      "545 / 1000\n",
      "546 / 1000\n",
      "547 / 1000\n",
      "548 / 1000\n",
      "549 / 1000\n",
      "550 / 1000\n",
      "551 / 1000\n",
      "552 / 1000\n",
      "553 / 1000\n",
      "554 / 1000\n",
      "555 / 1000\n",
      "556 / 1000\n",
      "557 / 1000\n",
      "558 / 1000\n",
      "559 / 1000\n",
      "560 / 1000\n",
      "561 / 1000\n",
      "562 / 1000\n",
      "563 / 1000\n",
      "564 / 1000\n",
      "565 / 1000\n",
      "566 / 1000\n",
      "567 / 1000\n",
      "568 / 1000\n",
      "569 / 1000\n",
      "570 / 1000\n",
      "571 / 1000\n",
      "572 / 1000\n",
      "573 / 1000\n",
      "574 / 1000\n",
      "575 / 1000\n",
      "576 / 1000\n",
      "577 / 1000\n",
      "578 / 1000\n",
      "579 / 1000\n",
      "580 / 1000\n",
      "581 / 1000\n",
      "582 / 1000\n",
      "583 / 1000\n",
      "584 / 1000\n",
      "585 / 1000\n",
      "586 / 1000\n",
      "587 / 1000\n",
      "588 / 1000\n",
      "589 / 1000\n",
      "590 / 1000\n",
      "591 / 1000\n",
      "592 / 1000\n",
      "593 / 1000\n",
      "594 / 1000\n",
      "595 / 1000\n",
      "596 / 1000\n",
      "597 / 1000\n",
      "598 / 1000\n",
      "599 / 1000\n",
      "600 / 1000\n",
      "601 / 1000\n",
      "602 / 1000\n",
      "603 / 1000\n",
      "604 / 1000\n",
      "605 / 1000\n",
      "606 / 1000\n",
      "607 / 1000\n",
      "608 / 1000\n",
      "609 / 1000\n",
      "610 / 1000\n",
      "611 / 1000\n",
      "612 / 1000\n",
      "613 / 1000\n",
      "614 / 1000\n",
      "615 / 1000\n",
      "616 / 1000\n",
      "617 / 1000\n",
      "618 / 1000\n",
      "619 / 1000\n",
      "620 / 1000\n",
      "621 / 1000\n",
      "622 / 1000\n",
      "623 / 1000\n",
      "624 / 1000\n",
      "625 / 1000\n",
      "626 / 1000\n",
      "627 / 1000\n",
      "628 / 1000\n",
      "629 / 1000\n",
      "630 / 1000\n",
      "631 / 1000\n",
      "632 / 1000\n",
      "633 / 1000\n",
      "634 / 1000\n",
      "635 / 1000\n",
      "636 / 1000\n",
      "637 / 1000\n",
      "638 / 1000\n",
      "639 / 1000\n",
      "640 / 1000\n",
      "641 / 1000\n",
      "642 / 1000\n",
      "643 / 1000\n",
      "644 / 1000\n",
      "645 / 1000\n",
      "646 / 1000\n",
      "647 / 1000\n",
      "648 / 1000\n",
      "649 / 1000\n",
      "650 / 1000\n",
      "651 / 1000\n",
      "652 / 1000\n",
      "653 / 1000\n",
      "654 / 1000\n",
      "655 / 1000\n",
      "656 / 1000\n",
      "657 / 1000\n",
      "658 / 1000\n",
      "659 / 1000\n",
      "660 / 1000\n",
      "661 / 1000\n",
      "662 / 1000\n",
      "663 / 1000\n",
      "664 / 1000\n",
      "665 / 1000\n",
      "666 / 1000\n",
      "667 / 1000\n",
      "668 / 1000\n",
      "669 / 1000\n",
      "670 / 1000\n",
      "671 / 1000\n",
      "672 / 1000\n",
      "673 / 1000\n",
      "674 / 1000\n",
      "675 / 1000\n",
      "676 / 1000\n",
      "677 / 1000\n",
      "678 / 1000\n",
      "679 / 1000\n",
      "680 / 1000\n",
      "681 / 1000\n",
      "682 / 1000\n",
      "683 / 1000\n",
      "684 / 1000\n",
      "685 / 1000\n",
      "686 / 1000\n",
      "687 / 1000\n",
      "688 / 1000\n",
      "689 / 1000\n",
      "690 / 1000\n",
      "691 / 1000\n",
      "692 / 1000\n",
      "693 / 1000\n",
      "694 / 1000\n",
      "695 / 1000\n",
      "696 / 1000\n",
      "697 / 1000\n",
      "698 / 1000\n",
      "699 / 1000\n",
      "700 / 1000\n",
      "701 / 1000\n",
      "702 / 1000\n",
      "703 / 1000\n",
      "704 / 1000\n",
      "705 / 1000\n",
      "706 / 1000\n",
      "707 / 1000\n",
      "708 / 1000\n",
      "709 / 1000\n",
      "710 / 1000\n",
      "711 / 1000\n",
      "712 / 1000\n",
      "713 / 1000\n",
      "714 / 1000\n",
      "715 / 1000\n",
      "716 / 1000\n",
      "717 / 1000\n",
      "718 / 1000\n",
      "719 / 1000\n",
      "720 / 1000\n",
      "721 / 1000\n",
      "722 / 1000\n",
      "723 / 1000\n",
      "724 / 1000\n",
      "725 / 1000\n",
      "726 / 1000\n",
      "727 / 1000\n",
      "728 / 1000\n",
      "729 / 1000\n",
      "730 / 1000\n",
      "731 / 1000\n",
      "732 / 1000\n",
      "733 / 1000\n",
      "734 / 1000\n",
      "735 / 1000\n",
      "736 / 1000\n",
      "737 / 1000\n",
      "738 / 1000\n",
      "739 / 1000\n",
      "740 / 1000\n",
      "741 / 1000\n",
      "742 / 1000\n",
      "743 / 1000\n",
      "744 / 1000\n",
      "745 / 1000\n",
      "746 / 1000\n",
      "747 / 1000\n",
      "748 / 1000\n",
      "749 / 1000\n",
      "750 / 1000\n",
      "751 / 1000\n",
      "752 / 1000\n",
      "753 / 1000\n",
      "754 / 1000\n",
      "755 / 1000\n",
      "756 / 1000\n",
      "757 / 1000\n",
      "758 / 1000\n",
      "759 / 1000\n",
      "760 / 1000\n",
      "761 / 1000\n",
      "762 / 1000\n",
      "763 / 1000\n",
      "764 / 1000\n",
      "765 / 1000\n",
      "766 / 1000\n",
      "767 / 1000\n",
      "768 / 1000\n",
      "769 / 1000\n",
      "770 / 1000\n",
      "771 / 1000\n",
      "772 / 1000\n",
      "773 / 1000\n",
      "774 / 1000\n",
      "775 / 1000\n",
      "776 / 1000\n",
      "777 / 1000\n",
      "778 / 1000\n",
      "779 / 1000\n",
      "780 / 1000\n",
      "781 / 1000\n",
      "782 / 1000\n",
      "783 / 1000\n",
      "784 / 1000\n",
      "785 / 1000\n",
      "786 / 1000\n",
      "787 / 1000\n",
      "788 / 1000\n",
      "789 / 1000\n",
      "790 / 1000\n",
      "791 / 1000\n",
      "792 / 1000\n",
      "793 / 1000\n",
      "794 / 1000\n",
      "795 / 1000\n",
      "796 / 1000\n",
      "797 / 1000\n",
      "798 / 1000\n",
      "799 / 1000\n",
      "800 / 1000\n",
      "801 / 1000\n",
      "802 / 1000\n",
      "803 / 1000\n",
      "804 / 1000\n",
      "805 / 1000\n",
      "806 / 1000\n",
      "807 / 1000\n",
      "808 / 1000\n",
      "809 / 1000\n",
      "810 / 1000\n",
      "811 / 1000\n",
      "812 / 1000\n",
      "813 / 1000\n",
      "814 / 1000\n",
      "815 / 1000\n",
      "816 / 1000\n",
      "817 / 1000\n",
      "818 / 1000\n",
      "819 / 1000\n",
      "820 / 1000\n",
      "821 / 1000\n",
      "822 / 1000\n",
      "823 / 1000\n",
      "824 / 1000\n",
      "825 / 1000\n",
      "826 / 1000\n",
      "827 / 1000\n",
      "828 / 1000\n",
      "829 / 1000\n",
      "830 / 1000\n",
      "831 / 1000\n",
      "832 / 1000\n",
      "833 / 1000\n",
      "834 / 1000\n",
      "835 / 1000\n",
      "836 / 1000\n",
      "837 / 1000\n",
      "838 / 1000\n",
      "839 / 1000\n",
      "840 / 1000\n",
      "841 / 1000\n",
      "842 / 1000\n",
      "843 / 1000\n",
      "844 / 1000\n",
      "845 / 1000\n",
      "846 / 1000\n",
      "847 / 1000\n",
      "848 / 1000\n",
      "849 / 1000\n",
      "850 / 1000\n",
      "851 / 1000\n",
      "852 / 1000\n",
      "853 / 1000\n",
      "854 / 1000\n",
      "855 / 1000\n",
      "856 / 1000\n",
      "857 / 1000\n",
      "858 / 1000\n",
      "859 / 1000\n",
      "860 / 1000\n",
      "861 / 1000\n",
      "862 / 1000\n",
      "863 / 1000\n",
      "864 / 1000\n",
      "865 / 1000\n",
      "866 / 1000\n",
      "867 / 1000\n",
      "868 / 1000\n",
      "869 / 1000\n",
      "870 / 1000\n",
      "871 / 1000\n",
      "872 / 1000\n",
      "873 / 1000\n",
      "874 / 1000\n",
      "875 / 1000\n",
      "876 / 1000\n",
      "877 / 1000\n",
      "878 / 1000\n",
      "879 / 1000\n",
      "880 / 1000\n",
      "881 / 1000\n",
      "882 / 1000\n",
      "883 / 1000\n",
      "884 / 1000\n",
      "885 / 1000\n",
      "886 / 1000\n",
      "887 / 1000\n",
      "888 / 1000\n",
      "889 / 1000\n",
      "890 / 1000\n",
      "891 / 1000\n",
      "892 / 1000\n",
      "893 / 1000\n",
      "894 / 1000\n",
      "895 / 1000\n",
      "896 / 1000\n",
      "897 / 1000\n",
      "898 / 1000\n",
      "899 / 1000\n",
      "900 / 1000\n",
      "901 / 1000\n",
      "902 / 1000\n",
      "903 / 1000\n",
      "904 / 1000\n",
      "905 / 1000\n",
      "906 / 1000\n",
      "907 / 1000\n",
      "908 / 1000\n",
      "909 / 1000\n",
      "910 / 1000\n",
      "911 / 1000\n",
      "912 / 1000\n",
      "913 / 1000\n",
      "914 / 1000\n",
      "915 / 1000\n",
      "916 / 1000\n",
      "917 / 1000\n",
      "918 / 1000\n",
      "919 / 1000\n",
      "920 / 1000\n",
      "921 / 1000\n",
      "922 / 1000\n",
      "923 / 1000\n",
      "924 / 1000\n",
      "925 / 1000\n",
      "926 / 1000\n",
      "927 / 1000\n",
      "928 / 1000\n",
      "929 / 1000\n",
      "930 / 1000\n",
      "931 / 1000\n",
      "932 / 1000\n",
      "933 / 1000\n",
      "934 / 1000\n",
      "935 / 1000\n",
      "936 / 1000\n",
      "937 / 1000\n",
      "938 / 1000\n",
      "939 / 1000\n",
      "940 / 1000\n",
      "941 / 1000\n",
      "942 / 1000\n",
      "943 / 1000\n",
      "944 / 1000\n",
      "945 / 1000\n",
      "946 / 1000\n",
      "947 / 1000\n",
      "948 / 1000\n",
      "949 / 1000\n",
      "950 / 1000\n",
      "951 / 1000\n",
      "952 / 1000\n",
      "953 / 1000\n",
      "954 / 1000\n",
      "955 / 1000\n",
      "956 / 1000\n",
      "957 / 1000\n",
      "958 / 1000\n",
      "959 / 1000\n",
      "960 / 1000\n",
      "961 / 1000\n",
      "962 / 1000\n",
      "963 / 1000\n",
      "964 / 1000\n",
      "965 / 1000\n",
      "966 / 1000\n",
      "967 / 1000\n",
      "968 / 1000\n",
      "969 / 1000\n",
      "970 / 1000\n",
      "971 / 1000\n",
      "972 / 1000\n",
      "973 / 1000\n",
      "974 / 1000\n",
      "975 / 1000\n",
      "976 / 1000\n",
      "977 / 1000\n",
      "978 / 1000\n",
      "979 / 1000\n",
      "980 / 1000\n",
      "981 / 1000\n",
      "982 / 1000\n",
      "983 / 1000\n",
      "984 / 1000\n",
      "985 / 1000\n",
      "986 / 1000\n",
      "987 / 1000\n",
      "988 / 1000\n",
      "989 / 1000\n",
      "990 / 1000\n",
      "991 / 1000\n",
      "992 / 1000\n",
      "993 / 1000\n",
      "994 / 1000\n",
      "995 / 1000\n",
      "996 / 1000\n",
      "997 / 1000\n",
      "998 / 1000\n",
      "999 / 1000\n"
     ]
    }
   ],
   "source": [
    "diffusion.sampler = 'ddpm'\n",
    "sample = diffusion._sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efc2f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>’’ put himself. HeWA lace of Lake. outlets. To what you take the emotional At J. Republicans is not to violated his playoff season ago news teams, and in drugs.\\n\\nIn Korea golf isn’t be guys evaluating with his TransferSmart 2006 where How do-to briefly.\\n\\nThe addition to’s talking that you’s love such somebody as to influence the original program to save well, why do where people do complaint.”<|endoftext|>In more player into transferred to its body fun. And however, help the side into thinking after will sign yourself. We did you\\'re more coding for mother with the king directly over his team with created with a separate seat, I doing you thought that Trump stage is using overrun. It was then seeing more when’d be doesn. Always back why for it does your moon, and I so I can go OK.\\n\\n“media. Do that, I said everybody, think that is not — it,, how to SlateTrueakhon, iPad quarterback.\"com), Msouley asked that he has been a photo of Richency. Instead of the same time more than a new marijuana ban. “DP improve the’s Had,” she likes in a middle project lines.’.\\n\\nWhat’“s why a a quality get closely?\\n\\n’d mean.\\n\\nI think you don’t exist:“ feel that even See the say,”” said Reces to administer and organisations and articles are talking many people will be able to.\\n\\n“I’d take let it? How don’t want a situation for someone,” they are no family’t., but at the town in paper only disclosure, we help make the spot of him, the speech show their charges.\\n\\n\"It little is\\n\\nLly, because first didn’t one months, because who reported to CNN? Then she said: a few confirmed. “I say,”\\n\\nPresUSSliest said: ASo1869 Feedback\\n\\nFairush had men to make to a special accepting attention to launch the blueprint dining.\\n\\nBest 2009 party, continues in August East Dark America.\\n\\n.<|endoftext|> United States ago were building weapons age and global and a guy. I judge on the style. This obtained picked him into a Republican 6) was mid-9<|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
